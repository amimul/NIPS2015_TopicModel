library(parallel)
source("~/workspace/R/Utilities/HelpersParallelization.R")
t1 <- Sys.time()
cluster <- startCluster()
models <- parLapply(cluster, ntopics, worker())
shutDownCluster(cluster)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
ntopics = seq(2, 25, by=1)
worker <- function() {
bindToEnv(objNames=c("documents", "vocab",
"G", "alpha", "eta"))
function(k) {
models = lapply(ntopics, function(k) topicmodels::LDA(dtm, k=k, method="Gibbs",
control=list(burnin=burnin, iter=iter, keep=keep)))
}
}
t1 <- Sys.time()
cluster <- startCluster()
models <- parLapply(cluster, ntopics, worker())
shutDownCluster(cluster)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
worker <- function() {
bindToEnv(objNames=c("dtm", "vocab",
"burnin", "iter", "keep"))
function(k) {
models = lapply(ntopics, function(k) topicmodels::LDA(dtm, k=k, method="Gibbs",
control=list(burnin=burnin, iter=iter, keep=keep)))
}
}
t1 <- Sys.time()
cluster <- startCluster()
models <- parLapply(cluster, ntopics, worker())
shutDownCluster(cluster)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
library(gplots)
library(Rmpfr)
library(dplyr)
library(stringi)
library(tm)
library(LDAvis)
library(parallel)
source("~/workspace/R/Utilities/HelpersParallelization.R")
load("docTermMatrix.rda")
load("records.rda")
## Set parameters for Gibbs sampling
burnin = 4000
iter = 2000
thin = 500
keep = 50
seed = list(2003, 5, 63, 100001, 765)
nstart = 5
best = TRUE
ntopics = seq(2, 25, by=1)
setwd("~/workspace/R/InflammationParkinson")
load("docTermMatrix.rda")
worker <- function() {
bindToEnv(objNames=c("dtm", "vocab",
"burnin", "iter", "keep"))
function(k) {
models = lapply(ntopics, function(k) topicmodels::LDA(dtm, k=k, method="Gibbs",
control=list(burnin=burnin, iter=iter, keep=keep)))
}
}
t1 <- Sys.time()
cluster <- startCluster()
models <- parLapply(cluster, ntopics, worker())
shutDownCluster(cluster)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
worker <- function() {
bindToEnv(objNames=c("dtm", "burnin", "iter", "keep"))
function(k) {
models = lapply(ntopics, function(k) topicmodels::LDA(dtm, k=k, method="Gibbs",
control=list(burnin=burnin, iter=iter, keep=keep)))
}
}
t1 <- Sys.time()
cluster <- startCluster()
models <- parLapply(cluster, ntopics, worker())
shutDownCluster(cluster)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
ntopics = seq(2, 25, by=1)
worker <- function() {
bindToEnv(objNames=c("dtm", "burnin", "iter", "keep"))
function(k) {
models = lapply(ntopics, function(k) topicmodels::LDA(dtm, k=k, method="Gibbs",
control=list(burnin=burnin, iter=iter, keep=keep)))
}
}
t1 <- Sys.time()
cluster <- startCluster()
models <- parLapply(cluster, ntopics, worker())
shutDownCluster(cluster)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
ntopics
worker <- function() {
bindToEnv(objNames=c("dtm", "burnin", "iter", "keep", "ntopics"))
function(k) {
models = lapply(ntopics, function(k) topicmodels::LDA(dtm, k=k, method="Gibbs",
control=list(burnin=burnin, iter=iter, keep=keep)))
}
}
t1 <- Sys.time()
cluster <- startCluster()
models <- parLapply(cluster, ntopics, worker())
shutDownCluster(cluster)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
names(models) <- ntopics
names(models)
logLiks = lapply(models, function(L)  L@logLiks[-c(1:(burnin/keep))])
harmMeans = sapply(logLiks, function(h) harmonicMean(h))
logLiks = lapply(models, function(L)  L@logLiks[-c(1:(burnin/keep))])
class(models)
models[[1]]
class(models)
length(models)
models[[1]]
models[[2]]
models[[3]]
worker <- function() {
bindToEnv(objNames=c("dtm", "burnin", "iter", "keep"))
function(k) {
models = lapply(ntopics, function(k) topicmodels::LDA(dtm, k=k, method="Gibbs",
control=list(burnin=burnin, iter=iter, keep=keep)))
}
}
t1 <- Sys.time()
cluster <- startCluster()
models <- parLapply(cluster, ntopics, worker())
shutDownCluster(cluster)
t2 <- Sys.time()
t2 - t1
ntopics = seq(2, 25, by=1)
library(topicmodels)
library(gplots)
library(Rmpfr)
library(dplyr)
library(stringi)
library(tm)
library(LDAvis)
library(parallel)
source("~/workspace/R/Utilities/HelpersParallelization.R")
load("docTermMatrix.rda")
load("records.rda")
## Set parameters for Gibbs sampling
burnin = 4000
iter = 2000
thin = 500
keep = 50
seed = list(2003, 5, 63, 100001, 765)
nstart = 5
best = TRUE
ntopics = seq(2, 9, by=1)
## Number of topics
# k can be estimated using a method called Model Selection by harmonic mean
# (see: http://stackoverflow.com/questions/21355156/topic-models-cross-validation-with-loglikelihood-or-perplexity)
# The diploma thesis shows a nice application of LDAs to estimate hot and cold topics over a certein time span
harmonicMean = function(logLikelihoods, precision=2000L) {
llMed = median(logLikelihoods)
as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
prec = precision) + llMed))))
}
# Run LDA using Gibbs sampling and harmonic mean maximization to estimate k
worker <- function() {
bindToEnv(objNames=c("dtm", "burnin", "iter", "keep"))
function(k) {
models = lapply(ntopics, function(k) topicmodels::LDA(dtm, k=k, method="Gibbs",
control=list(burnin=burnin, iter=iter, keep=keep)))
}
}
worker <- function() {
bindToEnv(objNames=c("dtm", "burnin", "iter", "keep"))
function(k) {
topicmodels::LDA(dtm, k=k, method="Gibbs",
control=list(burnin=burnin, iter=iter, keep=keep))
}
}
t1 <- Sys.time()
cluster <- startCluster()
models <- parLapply(cluster, ntopics, worker())
shutDownCluster(cluster)
t2 <- Sys.time()
t2 - t1
length(models)
names(models) <- ntopics
models[[1]]
models[[8]]
ntopics = seq(2, 25, by=1)
harmonicMean = function(logLikelihoods, precision=2000L) {
llMed = median(logLikelihoods)
as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
prec = precision) + llMed))))
}
# Run LDA using Gibbs sampling and harmonic mean maximization to estimate k
worker <- function() {
bindToEnv(objNames=c("dtm", "burnin", "iter", "keep"))
function(k) {
topicmodels::LDA(dtm, k=k, method="Gibbs",
control=list(burnin=burnin, iter=iter, keep=keep))
}
}
t1 <- Sys.time()
cluster <- startCluster()
models <- parLapply(cluster, ntopics, worker())
shutDownCluster(cluster)
t2 <- Sys.time()
t2 - t1
names(models) <- ntopics
length(models)
logLiks = lapply(models, function(L)  L@logLiks[-c(1:(burnin/keep))])
head(logLiks)
harmMeans = sapply(logLiks, function(h) harmonicMean(h))
k = which(harmMeans==max(harmMeans))
k
plot(ntopics, harmMeans, type = "l")
model = models[[k]]
records = data.frame(PubMedID=PMID(records), Title=ArticleTitle(records),
Abstract=AbstractText(records), Year=YearPubmed(records))
model.topics = as.data.frame(topics(model))
model.topics = data.frame(PubMedID=rownames(model.topics), Topic=model.topics[ ,1])
model.terms = as.data.frame(terms(model, 50))
topicProbabilities = as.data.frame(model@gamma)
rownames(topicProbabilities) = model@documents
colnames(topicProbabilities) = colnames(model.terms)
model.topics$Posterior = apply(topicProbabilities, 1, max)
assignedRecords = merge(x=model.topics, y=records, by="PubMedID")
model
getRecordByTopic = function(i) {
topic = assignedRecords[which(assignedRecords$Topic==as.character(i)), ]
return(topic)
}
topic1 = getRecordByTopic(1)
topic2 = getRecordByTopic(2)
topic3 = getRecordByTopic(3)
topic4 = getRecordByTopic(4)
topic5 = getRecordByTopic(5)
knitr::kable(head(topic1[, 1:4], 10), digits = 2, caption = "Topic 1")
knitr::kable(head(topic2[, 1:4], 10), digits = 2, caption = "Topic 2")
knitr::kable(head(topic3[, 1:4], 10), digits = 2, caption = "Topic 3")
knitr::kable(head(topic4[, 1:4], 10), digits = 2, caption = "Topic 4")
knitr::kable(head(topic5[, 1:4], 10), digits = 2, caption = "Topic 5")
setwd("~/workspace/R/NIPS_LDA")
library(RISmed)
library(tm)
library(data.table)
library(tm)
library(data.table)
library(topicmodels)
library(gplots)
library(Rmpfr)
library(wordcloud)
library(pheatmap)
library(parallel)
papers = fread("~/Datasets/kaggle/NIPS2015_papers/Papers.csv")
corpus = Corpus(VectorSource(papers$Abstract))
corpus
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removePunctuation, preserve_intra_word_dashes = TRUE)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, toSpace, "\"")
corpus <- tm_map(corpus, toSpace, "’")
corpus <- tm_map(corpus, toSpace, "•")
corpus = tm_map(corpus, stripWhitespace)
writeLines(as.character(corpus[[1]]))
corpus = Corpus(VectorSource(papers$Abstract))
corpus = tm_map(corpus, stripWhitespace)
writeLines(as.character(corpus[[1]]))
toSpace = content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
papers = fread("~/Datasets/kaggle/NIPS2015_papers/Papers.csv")
corpus = Corpus(VectorSource(papers$Abstract))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
writeLines(as.character(corpus[[1]]))
corpus = tm_map(corpus, stripWhitespace)
writeLines(as.character(corpus[[1]]))
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.95)
rownames(dtm) = papers$Id
rowTotals = apply(dtm, 1, sum)
dtm = dtm[rowTotals > 0, ]
### Collapse matrix by summing over columns
freq = colSums(as.matrix(dtm))
### Length should be total number of terms
length(freq)
### List all terms in decreasing order of freq
freq = sort(freq, decreasing=TRUE)
freq = data.frame(word=names(freq), freq=freq)
rownames(freq) = NULL
head(freq, 10)
pal = brewer.pal(8,"Dark2")
wordcloud(words = freq$word, freq=freq$freq, min.freq = 100,
random.order=FALSE, rot.per=.15, colors=pal)
dtm = removeSparseTerms(dtm, 0.99)
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.99)
rownames(dtm) = papers$Id
rowTotals = apply(dtm, 1, sum)
dtm = dtm[rowTotals > 0, ]
### Collapse matrix by summing over columns
freq = colSums(as.matrix(dtm))
### Length should be total number of terms
length(freq)
### List all terms in decreasing order of freq
freq = sort(freq, decreasing=TRUE)
freq = data.frame(word=names(freq), freq=freq)
rownames(freq) = NULL
head(freq, 10)
pal = brewer.pal(8,"Dark2")
wordcloud(words = freq$word, freq=freq$freq, min.freq = 100,
random.order=FALSE, rot.per=.15, colors=pal)
burnin = 4000
iter = 2000
thin = 500
keep = 50
seed = list(2003, 5, 63, 100001, 765)
nstart = 5
best = TRUE
harmonicMean = function(logLikelihoods, precision=2000L) {
llMed = median(logLikelihoods)
as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
prec = precision) + llMed))))
}
ntopics = seq(2, 25, by=1)
worker <- function() {
bindToEnv(objNames=c("dtm", "burnin", "iter", "keep"))
function(k) {
topicmodels::LDA(dtm, k=k, method="Gibbs",
control=list(burnin=burnin, iter=iter, keep=keep))
}
}
t1 <- Sys.time()
cluster <- startCluster()
models <- parLapply(cluster, ntopics, worker())
shutDownCluster(cluster)
t2 <- Sys.time()
t2 - t1
bindToEnv <- function(bindTargetEnv=parent.frame(), objNames, doNotRebind=c()) {
# Bind the values into environment
# and switch any functions to this environment!
for(var in objNames) {
val <- get(var, envir=parent.frame())
if(is.function(val) && (!(var %in% doNotRebind))) {
# replace function's lexical environment with our target (DANGEROUS)
environment(val) <- bindTargetEnv
}
# assign object to target environment, only after any possible alteration
assign(var, val, envir=bindTargetEnv)
}
}
startCluster <- function(cores=detectCores()) {
cluster <- makeCluster(cores)
return(cluster)
}
shutDownCluster <- function(cluster) {
if(!is.null(cluster)) {
stopCluster(cluster)
cluster <- c()
}
}
worker <- function() {
bindToEnv(objNames=c("dtm", "burnin", "iter", "keep"))
function(k) {
topicmodels::LDA(dtm, k=k, method="Gibbs",
control=list(burnin=burnin, iter=iter, keep=keep))
}
}
t1 <- Sys.time()
cluster <- startCluster()
models <- parLapply(cluster, ntopics, worker())
shutDownCluster(cluster)
t2 <- Sys.time()
t2 - t1
logLiks = lapply(models, function(L)  L@logLiks[-c(1:(burnin/keep))])
harmMeans = sapply(logLiks, function(h) harmonicMean(h))
k = which(harmMeans==max(harmMeans))
plot(ntopics, harmMeans, type = "l")
model = models[[k]]
as.data.frame(topics(model))
as.data.frame(terms(model, 20))
model.terms = as.data.frame(terms(model, 20))
stemCompletion(model.terms$`Topic 1`, dtm)
stemCompletion(model.terms$`Topic 1`, corpus)
topicProbabilities = as.data.frame(model@gamma)
rownames(topicProbabilities) = model@documents
colnames(topicProbabilities) = colnames(model.terms)
head(topicProbabilities)
colors <- colorRampPalette(c("black", "white", "red"))(15)
pheatmap(as.matrix(topicProbabilities), scale="row", cluster_rows=T, cluster_cols=T,
clustering_distance_rows = "euclidean",
clustering_method = "complete",
color=colors)
model.topics$Posterior = apply(topicProbabilities, 1, max)
model.topics = as.data.frame(topics(model))
model.topics$Posterior = apply(topicProbabilities, 1, max)
assignedRecords = merge(x=model.topics, y=papers, by="Id")
papers$Id
head(model.topics)
model.topics = data.frame(Id=rownames(model.topics), Topic=model.topics[ ,1])
model.topics$Posterior = apply(topicProbabilities, 1, max)
assignedRecords = merge(x=model.topics, y=papers, by="Id")
assignedRecords = assignedRecords[order(assignedRecords$Topic,-assignedRecords$Posterior), ]
getRecordByTopic = function(i) {
topic = assignedRecords[which(assignedRecords$Topic==as.character(i)), ]
return(topic)
}
topic1 = getRecordByTopic(1)
topic2 = getRecordByTopic(2)
topic3 = getRecordByTopic(3)
topic4 = getRecordByTopic(4)
topic5 = getRecordByTopic(5)
knitr::kable(head(topic1[, 1:4], 10), digits = 2, caption = "Topic 1")
knitr::kable(head(topic2[, 1:4], 10), digits = 2, caption = "Topic 2")
knitr::kable(head(topic3[, 1:4], 10), digits = 2, caption = "Topic 3")
knitr::kable(head(topic4[, 1:4], 10), digits = 2, caption = "Topic 4")
knitr::kable(head(topic5[, 1:4], 10), digits = 2, caption = "Topic 5")
head(topic5)
head(topic5[, 1:4], 10)
# read in some stopwords:
library(tm)
library(data.table)
library(LDAvis)
source("~/workspace/R/Utilities/HelpersParallelization.R")
papers = fread("~/Datasets/kaggle/NIPS2015_papers/Papers.csv")
docs = papers$Abstract
stop_words <- stopwords("SMART")
# pre-processing:
docs <- gsub("[[:punct:]]", " ", docs)  # replace punctuation with space
docs <- gsub("[[:cntrl:]]", " ", docs)  # replace control characters with space
docs <- gsub("^[[:space:]]+", "", docs) # remove whitespace at beginning of documents
docs <- gsub("[[:space:]]+$", "", docs) # remove whitespace at end of documents
docs <- tolower(docs)  # force to lowercase
# tokenize on space and output as a list:
doc.list <- strsplit(docs, "[[:space:]]+")
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)
# now put the documents into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]
# MCMC and model tuning parameters:
K <- 20
G <- 5000
alpha <- 0.02
eta <- 0.02
ntopics = seq(2, 9, by=1)
# Fit the model:
library(lda)
set.seed(357)
worker <- function() {
bindToEnv(objNames=c("documents", "vocab",
"G", "alpha", "eta"))
function(k) {
lda::lda.collapsed.gibbs.sampler(documents = documents, K = k, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
}
}
t1 <- Sys.time()
cluster <- startCluster()
models <- parLapply(cluster, ntopics, worker())
shutDownCluster(cluster)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
names(models) <- ntopics
harmonicMean = function(logLikelihoods, precision=2000L) {
llMed = median(logLikelihoods)
as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
prec = precision) + llMed))))
}
logLiks = lapply(models, function(L)  L$log.likelihoods)
harmMeans = sapply(logLiks, function(h) harmonicMean(h))
k = which(harmMeans==max(harmMeans))
plot(ntopics, harmMeans, type = "l")
model = models[[k]]
model = models[[4]]
theta <- t(apply(model$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(model$topics) + eta, 2, function(x) x/sum(x)))
modelParams <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
# create the JSON object to feed the visualization:
json <- createJSON(phi = modelParams$phi,
theta = modelParams$theta,
doc.length = modelParams$doc.length,
vocab = modelParams$vocab,
term.frequency = modelParams$term.frequency)
serVis(json, open.browser = TRUE)
